{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from io import StringIO\n",
    "import seaborn as sns\n",
    "import re  \n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 1. Set up urls and functions\n",
    "# =============================================================================\n",
    "\n",
    "# Repo containing snapshots for ILI incidence\n",
    "repo_url = \"https://github.com/european-modelling-hubs/RespiCast-SyndromicIndicators/tree/main/target-data/ERVISS/snapshots\"\n",
    "\n",
    "# Base URL for raw snapshot files\n",
    "raw_base_url = \"https://raw.githubusercontent.com/european-modelling-hubs/RespiCast-SyndromicIndicators/main/target-data/ERVISS/snapshots\"\n",
    "\n",
    "# URL for the final reported data\n",
    "final_data_url = \"https://raw.githubusercontent.com/european-modelling-hubs/RespiCast-SyndromicIndicators/refs/heads/main/target-data/ERVISS/latest-ILI_incidence.csv\"\n",
    "\n",
    "# Get all snapshot file names \n",
    "def get_all_snapshot_files(base_url):\n",
    "    response = requests.get(base_url)\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        # Find CSV files with \"-ILI_incidence.csv\"\n",
    "        return [\n",
    "            link.get('href').split('/')[-1]\n",
    "            for link in soup.find_all('a', href=True)\n",
    "            if link.get('href').endswith('-ILI_incidence.csv')\n",
    "        ]\n",
    "    else:\n",
    "        raise Exception(f\"Error loading file: {base_url}\")\n",
    "\n",
    "# Load CSV from a URL\n",
    "def load_csv_from_url(url):\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        return pd.read_csv(StringIO(response.text))\n",
    "    else:\n",
    "        raise Exception(f\"Error loading file: {url}\")\n",
    "    \n",
    "# Get the snapshot date from the file name \n",
    "def extract_date_from_filename(filename):\n",
    "    match = re.match(r'(\\d{4}-\\d{2}-\\d{2})', filename) # (format: YYYY-MM-DD)\n",
    "    return match.group(1) if match else None\n",
    "\n",
    "# Compute the week in season from a given date \n",
    "def calculate_week_in_season(date):\n",
    "    season_start = pd.Timestamp(year=date.year, month=9, day=1)\n",
    "    if date.month < 9:\n",
    "        season_start = pd.Timestamp(year=date.year - 1, month=9, day=1)\n",
    "    weeks_in_season = (date - season_start).days // 7 + 1\n",
    "    return weeks_in_season\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38 snapshot file found: ['2024-10-11-ILI_incidence.csv', '2024-10-11-ILI_incidence.csv', '2024-10-18-ILI_incidence.csv', '2024-10-18-ILI_incidence.csv', '2024-10-25-ILI_incidence.csv', '2024-10-25-ILI_incidence.csv', '2024-11-08-ILI_incidence.csv', '2024-11-08-ILI_incidence.csv', '2024-11-15-ILI_incidence.csv', '2024-11-15-ILI_incidence.csv', '2024-11-22-ILI_incidence.csv', '2024-11-22-ILI_incidence.csv', '2024-11-29-ILI_incidence.csv', '2024-11-29-ILI_incidence.csv', '2024-12-06-ILI_incidence.csv', '2024-12-06-ILI_incidence.csv', '2024-12-13-ILI_incidence.csv', '2024-12-13-ILI_incidence.csv', '2024-12-20-ILI_incidence.csv', '2024-12-20-ILI_incidence.csv', '2025-01-03-ILI_incidence.csv', '2025-01-03-ILI_incidence.csv', '2025-01-10-ILI_incidence.csv', '2025-01-10-ILI_incidence.csv', '2025-01-17-ILI_incidence.csv', '2025-01-17-ILI_incidence.csv', '2025-01-24-ILI_incidence.csv', '2025-01-24-ILI_incidence.csv', '2025-01-31-ILI_incidence.csv', '2025-01-31-ILI_incidence.csv', '2025-02-07-ILI_incidence.csv', '2025-02-07-ILI_incidence.csv', '2025-02-14-ILI_incidence.csv', '2025-02-14-ILI_incidence.csv', '2025-02-21-ILI_incidence.csv', '2025-02-21-ILI_incidence.csv', '2025-02-28-ILI_incidence.csv', '2025-02-28-ILI_incidence.csv']\n",
      "          target location  truth_date year_week   value  \\\n",
      "0  ILI incidence       AT  2024-04-07  2024-W14  1512.6   \n",
      "1  ILI incidence       AT  2024-03-31  2024-W13  1629.1   \n",
      "2  ILI incidence       AT  2024-03-24  2024-W12  2179.8   \n",
      "3  ILI incidence       AT  2024-03-17  2024-W11  1798.0   \n",
      "4  ILI incidence       AT  2024-03-10  2024-W10  1790.0   \n",
      "\n",
      "                    source_file  \n",
      "0  2024-10-11-ILI_incidence.csv  \n",
      "1  2024-10-11-ILI_incidence.csv  \n",
      "2  2024-10-11-ILI_incidence.csv  \n",
      "3  2024-10-11-ILI_incidence.csv  \n",
      "4  2024-10-11-ILI_incidence.csv  \n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# 2. Get snapshot files and load data\n",
    "# =============================================================================\n",
    "\n",
    "# Get all snapshot names\n",
    "snapshot_files = get_all_snapshot_files(repo_url)\n",
    "print(f\"{len(snapshot_files)} snapshot file found: {snapshot_files}\")\n",
    "\n",
    "all_snapshot_data = pd.DataFrame()\n",
    "\n",
    "# Create dataframe with all snapshot files\n",
    "final_data = load_csv_from_url(final_data_url)\n",
    "\n",
    "# Iterate trough snapshot files and keep data \n",
    "for file_name in snapshot_files:\n",
    "    snapshot_url = f\"{raw_base_url}/{file_name}\"\n",
    "    try:\n",
    "        snapshot_data = load_csv_from_url(snapshot_url)\n",
    "        snapshot_data['source_file'] = file_name  # Add column to identify file \n",
    "        all_snapshot_data = pd.concat([all_snapshot_data, snapshot_data], ignore_index=True)\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading file {snapshot_url}: {e}\")\n",
    "\n",
    "print(all_snapshot_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 3. Convert date columns and filter data \n",
    "# =============================================================================\n",
    "\n",
    "\n",
    "# Convert date in datetime \n",
    "all_snapshot_data['truth_date'] = pd.to_datetime(all_snapshot_data['truth_date'])\n",
    "final_data['truth_date'] = pd.to_datetime(final_data['truth_date'])\n",
    "\n",
    "# Filter data from a fixed date \n",
    "all_snapshot_data = all_snapshot_data[all_snapshot_data['truth_date'] >= '2024-10']\n",
    "final_data = final_data[final_data['truth_date'] >= '2024-10']\n",
    "\n",
    "# Filter to include only dates in snapshot \n",
    "common_dates = all_snapshot_data['truth_date'].unique()\n",
    "filtered_final_data = final_data[final_data['truth_date'].isin(common_dates)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing country: BE\n",
      "Processing country: CZ\n",
      "Processing country: DK\n",
      "Processing country: GR\n",
      "Processing country: HU\n",
      "Processing country: LV\n",
      "Processing country: LT\n",
      "Processing country: LU\n",
      "Processing country: MT\n",
      "Processing country: NL\n",
      "Processing country: NO\n",
      "Processing country: PL\n",
      "Processing country: RO\n",
      "Processing country: SI\n",
      "Processing country: AT\n",
      "Processing country: HR\n",
      "Processing country: FR\n",
      "Processing country: IE\n",
      "Processing country: EE\n",
      "Processing country: IS\n",
      "Processing country: IT\n",
      "Processing country: FI\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# 4. Process data for each country \n",
    "# =============================================================================\n",
    "# For each country extact incidence value,\n",
    "# check reevision status,\n",
    "# compute age of data \n",
    "\n",
    "final_dataset = []\n",
    "\n",
    "for country in all_snapshot_data['location'].unique():\n",
    "    print(f\"Processing country: {country}\")\n",
    "\n",
    "    # Select data for current country \n",
    "    country_snapshot_data = all_snapshot_data[all_snapshot_data['location'] == country].copy()\n",
    "    country_final_data = filtered_final_data[filtered_final_data['location'] == country].copy()\n",
    "\n",
    "    # Extact source file date from file name \n",
    "    country_snapshot_data['source_file_date'] = country_snapshot_data['source_file'].apply(extract_date_from_filename)\n",
    "    country_snapshot_data['source_file_date'] = pd.to_datetime(country_snapshot_data['source_file_date'], errors='coerce')\n",
    "    country_snapshot_data = country_snapshot_data.dropna(subset=['source_file_date'])\n",
    "    \n",
    "    # Check missing data\n",
    "    if country_snapshot_data.empty or country_final_data.empty:\n",
    "        print(f\"ERROR: Missing data for country {country}\")\n",
    "        continue\n",
    "\n",
    "    for idx, snapshot_row in country_snapshot_data.iterrows():\n",
    "        date = pd.to_datetime(snapshot_row['truth_date'])\n",
    "\n",
    "        # Incidence value\n",
    "        snapshot_value = snapshot_row['value']\n",
    "\n",
    "        #Revision status\n",
    "        final_value = country_final_data[country_final_data['truth_date'] == date]['value'].iloc[0]\n",
    "        revised = 1 if not np.isclose(snapshot_value, final_value, atol=1e-6) else 0\n",
    "\n",
    "        # Age of data (weeks after original report)\n",
    "        weeks_after = (snapshot_row['source_file_date'] - date).days // 7\n",
    "\n",
    "\n",
    "        final_dataset.append({\n",
    "            'truth_date': date,\n",
    "            'country': country,\n",
    "            'snapshot_date': snapshot_row['source_file_date'],\n",
    "            'value': snapshot_value,\n",
    "            'age': weeks_after,\n",
    "            'revision_status': revised,\n",
    "            'week_in_season': calculate_week_in_season(date),\n",
    "            'revision_amount': final_value - snapshot_value\n",
    "        })\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Revision dataset for season 24/25: 8958 lines saved in ILI_datasets/1_ILI_revisions_dataset_season_24_25_prova.csv\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# 5 Save final dataset to a csv file\n",
    "# =============================================================================\n",
    "\n",
    "folder_path = \"ILI_datasets\"  # Folder name\n",
    "os.makedirs(folder_path, exist_ok=True)  # Create folder\n",
    "\n",
    "file_path = os.path.join(folder_path, \"1_ILI_revisions_dataset_season_24_25_prova.csv\")\n",
    "\n",
    "# Save final dataset \n",
    "if final_dataset:\n",
    "    final_df = pd.DataFrame(final_dataset)\n",
    "    final_df.to_csv(file_path, index=False)\n",
    "    print(f\"Revision dataset for season 24/25: {len(final_dataset)} lines saved in {file_path}\")\n",
    "else:\n",
    "    print(\"ERROR: Revision dataset for season 24/25 is empty.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
